{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MSDS 7331 - Case Study 1 - Superconducting Materials\r\n",
    "Daniel Crouthamel\r\n",
    "Sophia Wu\r\n",
    "Fabio Savorgnan\r\n",
    "Bo Yun\r\n",
    "\r\n",
    "## Business Understanding\r\n",
    "--\r\n",
    "You should always state the objective at the beginning of every case (a guideline you should follow in real life as well) and provide some initial \"Business Understanding\" statements (i.e., what is trying to be solved for and why might it be important)\r\n",
    "--\r\n",
    "\r\n",
    "**Objective:** \r\n",
    "The objective of this case study is to explore Linerar Regression with L1 and L2 regularization, and the impact to predicting the critical temperature of a superconductor. Additionally, feature importance is also investigated with the best model. Three models will be considered.\r\n",
    "\r\n",
    "* Lasso (L1)\r\n",
    "* Ridge (L2)\r\n",
    "* Elastic Net (L1 and L2).\r\n",
    "\r\n",
    "**Todo!!**\r\n",
    "Summarize/background on L1/L2, the affect of alpha, l1 ratio, the hyperparameters.\r\n",
    "\r\n",
    "Grid search will be performed on each model to find the optimal hyperparameters. We'll then use the optimal hyperparameters for each model and then determine which model performs the best. Along the way will explore the importance of normalization and scaling and condlude with a summary of the most important features."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Evaluation and Engineering\r\n",
    "--\r\n",
    "Summarize the data being used in the case using appropriate mediums (charts, graphs, tables); address questions such as: Are there missing values? Which variables are needed (which ones are not)? What assumptions or conclusions are you drawing that need to be relayed to your audience?\r\n",
    "--\r\n",
    "\r\n",
    "Below we'll load our supoerconductor data and then perform a quick data exploration. Our data consists of two files which were merged together. Some initial obervations are:\r\n",
    "\r\n",
    "* There is no missing data\r\n",
    "* We have features with constant values\r\n",
    "* There is one string object in the dataframe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "# Load Data\r\n",
    "data_train = pd.read_csv('./data/train.csv')\r\n",
    "data_materials = pd.read_csv('./data/unique_m.csv')\r\n",
    "\r\n",
    "# Drop the duplicate column 'critical_temp' in the first frame\r\n",
    "data_train = data_train.drop(['critical_temp'], axis=1)\r\n",
    "\r\n",
    "# Merge the two frames\r\n",
    "data = pd.merge(data_train, data_materials, left_index=True, right_index=True)\r\n",
    "\r\n",
    "# Data frame to csv drop index\r\n",
    "data.to_csv('./data/super_conducter_data.csv', index=False)\r\n",
    "\r\n",
    "# Print out some typicaly descriptive statistics\r\n",
    "print(\"\")\r\n",
    "data.info()\r\n",
    "data.describe()\r\n",
    "\r\n",
    "# Do we have missing data?\r\n",
    "print(\"\")\r\n",
    "print(\"Missing Data?\", data.columns[data.isnull().any().values])\r\n",
    "\r\n",
    "# Columns with a Constant value\r\n",
    "print(\"\")\r\n",
    "print(\"Columns that have the same value\", data.columns[data.nunique() <= 1].values)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21263 entries, 0 to 21262\n",
      "Columns: 169 entries, number_of_elements to material\n",
      "dtypes: float64(156), int64(12), object(1)\n",
      "memory usage: 27.4+ MB\n",
      "\n",
      "Missing Data? Index([], dtype='object')\n",
      "\n",
      "Columns that have the same value ['He' 'Ne' 'Ar' 'Kr' 'Xe' 'Pm' 'Po' 'At' 'Rn']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The string feature will be removed from our data set. This appears to be a name, and some values are duplicated. We felt it was save to remove. Addtionally, the features identified above with constant values will be removed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Drop columns with constant values\r\n",
    "data.drop(columns=['material', 'He', 'Ne', 'Ar', 'Kr', 'Xe', 'Pm', 'Po', 'At', 'Rn'], inplace=True)\r\n",
    "print(data.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(21263, 159)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code below is used to create a pandas profile, which is an easy way to see summary statistics for each feature. The html file will be provided as part of the case study submission. Some findings from that report are:\r\n",
    "\r\n",
    "* Confirms no missing values\r\n",
    "* There are no negative values\r\n",
    "* Varying distrubutions for the features, some bell shaped, others poisson, etc.\r\n",
    "* We have features with skewness and outliers.\r\n",
    "\r\n",
    "To run the code, uncomment the last 3 lines of code below and run the cell. The first two lines indicate what needs to be installed. You will need to use an older version of pandas."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "## install pandas 1.2.4\r\n",
    "## pip install pandas-profiling==2.8.0\r\n",
    "\r\n",
    "# from pandas_profiling import ProfileReport\r\n",
    "# profile = ProfileReport(data, title=\"Pandas Profiling Report\", minimal=True)\r\n",
    "# profile.to_file(output_file=\"PandasProfile.html\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modeling Preparations\r\n",
    "--\r\n",
    "Which methods are you proposing to utilize to solve the problem?  Why is this method appropriate given the business objective? How will you determine if your approach is useful (or how will you differentiate which approach is more useful than another)?  More specifically, what evaluation metrics are most useful given that the problem is a regression one (ex., RMSE, logloss, MAE, etc.)?\r\n",
    "--\r\n",
    "\r\n",
    "As mentioned above, we'll be using grid search on three different models. For each model we'll use R2 as a cross validation metric, which can can explain how much of the variance is capatured by the model. We'll then use MAE (Mean Absolute Error) to differentiate between if Lasso, Ridge, or Elastic Net performs better."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Building and Evaluation\r\n",
    "--\r\n",
    "In this case, your primary task is to build a linear regression model using L1 or L2 regularization (or both) to predict the critical temperature and will involve the following steps:\r\n",
    "\r\n",
    "- Specify your sampling methodology\r\n",
    "- Setup your model(s) - specifying the regularization type chosen and including the parameters utilized by the model\r\n",
    "- Analyze your model's performance - referencing your chosen evaluation metric (including supplemental visuals and analysis where appropriate)\r\n",
    "--\r\n",
    "\r\n",
    "The code below creates 3 models and performs grid search on each model. For Lasso and Ridge, we'll vary the hyperparameter alpha. For Elastic Net, we'll vary the hyperparameters alpha and l1_ratio. Ranges are shown below in the code.\r\n",
    "\r\n",
    "Normalization and scaling are performed on the data. There are several different ones that can be used, but in the end we decided to go with RobustScaler, which scales features using statistics that are robust to outliers.\r\n",
    "\r\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html\r\n",
    "\r\n",
    "We additionally use a Pipeline to define the order of steps. By using a Pipeline with GridSearchCV we only scale the data in the training set. The test sets in each fold are then scaled using the same scaler.\r\n",
    "\r\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html\r\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\r\n",
    "https://stats.stackexchange.com/questions/445259/combining-pca-feature-scaling-and-cross-validation-without-training-test-data\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.preprocessing import RobustScaler\r\n",
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "from sklearn.linear_model import Lasso\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "from sklearn.linear_model import ElasticNet\r\n",
    "from sklearn.linear_model import Ridge\r\n",
    "from sklearn.pipeline import make_pipeline\r\n",
    "\r\n",
    "X = data.drop(columns=['critical_temp']).copy(deep=True)\r\n",
    "y = data.loc[:,'critical_temp'].copy(deep=True)\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test =\\\r\n",
    "    train_test_split(X, y,\r\n",
    "    test_size=0.2,\r\n",
    "    random_state=1)\r\n",
    "\r\n",
    "lasso_pipe_svc = make_pipeline(RobustScaler(), Lasso(random_state=1))\r\n",
    "ridge_pipe_svc = make_pipeline(RobustScaler(), Ridge(random_state=1))\r\n",
    "elastic_pipe_svc = make_pipeline(RobustScaler(), ElasticNet(random_state=1))\r\n",
    "\r\n",
    "param_range = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 10, 100, 1000, 10000]\r\n",
    "param_l1_ratio = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\r\n",
    "\r\n",
    "param_grid_lasso = [{'lasso__alpha': param_range}]\r\n",
    "param_grid_ridge = [{'ridge__alpha': param_range}]\r\n",
    "param_grid_elastic = [{'elasticnet__alpha': param_range, 'elasticnet__l1_ratio': param_l1_ratio}]\r\n",
    "\r\n",
    "gs_lasso = GridSearchCV(estimator=lasso_pipe_svc, param_grid=param_grid_lasso, scoring='r2', cv=5, n_jobs=-1)\r\n",
    "gs_lasso.fit(X_train, y_train)\r\n",
    "\r\n",
    "gs_ridge = GridSearchCV(estimator=ridge_pipe_svc, param_grid=param_grid_ridge, scoring='r2', cv=5, n_jobs=-1)\r\n",
    "gs_ridge.fit(X_train, y_train)\r\n",
    "\r\n",
    "gs_elastic = GridSearchCV(estimator=elastic_pipe_svc, param_grid=param_grid_elastic, scoring='r2', cv=5, n_jobs=-1)\r\n",
    "gs_elastic.fit(X_train, y_train)\r\n",
    "\r\n",
    "print(\"Lasso\")\r\n",
    "print(gs_lasso.best_score_)\r\n",
    "print(gs_lasso.best_params_)\r\n",
    "print(\"\")\r\n",
    "\r\n",
    "print(\"Ridge\")\r\n",
    "print(gs_ridge.best_score_)\r\n",
    "print(gs_ridge.best_params_)\r\n",
    "print(\"\")\r\n",
    "\r\n",
    "print(\"Elastic\")\r\n",
    "print(gs_elastic.best_score_)\r\n",
    "print(gs_elastic.best_params_)\r\n",
    "print(\"\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lasso\n",
      "0.7129520975572087\n",
      "{'lasso__alpha': 0.3}\n",
      "\n",
      "Ridge\n",
      "0.706531275524331\n",
      "{'ridge__alpha': 1000}\n",
      "\n",
      "Elastic\n",
      "0.7083853891920866\n",
      "{'elasticnet__alpha': 0.3, 'elasticnet__l1_ratio': 0.9}\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Above we see that each model performs almost identically on the training data. Recall that L1 is used for Lasso, and L2 is used for Ridget. Elastic Net uses both. The optimal l1_ration hyperparameter found for ElasticNet is 0.9. It's moving into the direction of Lasso. If the ratio is 1, it's Lasso, if it's 0, it's Ridge.\r\n",
    "\r\n",
    "Additionally, the optimal alpha found for Ridge is 1000, which is quite high. It's trying to penalize cofficients harder, which means we'll see more coefficients that are close to zero.\r\n",
    "\r\n",
    "Next we'll perform predictions on our test data. Note that because we are using a pipeline, the scaler is only applied to the test data. This ensures that we have no leakage of information from the training data.\r\n",
    "https://stackoverflow.com/questions/35388647/how-to-use-gridsearchcv-output-for-a-scikit-prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from sklearn import metrics\r\n",
    "\r\n",
    "# Note the X_test gets run through the pipeline above! Very important, it means that the scaler is also run on the test data\r\n",
    "y_lasso_pred = gs_lasso.predict(X_test)\r\n",
    "y_ridge_pred = gs_ridge.predict(X_test)\r\n",
    "y_elastic_pred = gs_elastic.predict(X_test)\r\n",
    "\r\n",
    "print(\"Lasso\")\r\n",
    "print(\"R2 ->\", metrics.r2_score(y_test, y_lasso_pred))\r\n",
    "print(\"MAE ->\", metrics.mean_absolute_error(y_test, y_lasso_pred))\r\n",
    "\r\n",
    "print(\"Ridge\")\r\n",
    "print(\"R2 ->\", metrics.r2_score(y_test, y_ridge_pred))\r\n",
    "print(\"MAE ->\", metrics.mean_absolute_error(y_test, y_ridge_pred))\r\n",
    "print(\"\")\r\n",
    "\r\n",
    "print(\"Elastic\")\r\n",
    "print(\"R2 ->\", metrics.r2_score(y_test, y_elastic_pred))\r\n",
    "print(\"MAE ->\", metrics.mean_absolute_error(y_test, y_elastic_pred))\r\n",
    "print(\"\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lasso\n",
      "R2 -> 0.7197254814065868\n",
      "MAE -> 13.660714491348584\n",
      "Ridge\n",
      "R2 -> 0.7264906957630273\n",
      "MAE -> 13.359929361601823\n",
      "\n",
      "Elastic\n",
      "R2 -> 0.7146988481193569\n",
      "MAE -> 13.811785860925973\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Above we see that the R2 values are consistent with what we found on the training data. However, the Ridge model performans marginally better. In this case I'd be inclined to use the Lasso model because of the reduced number of features. Below we output the ABSOLUTE value of each coefficient, in sorted fashion. This helps to give an indication of which features are important. The sign doesn't matter."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "lasso_weights = {data.columns[k]:abs(v) for k, v in enumerate(gs_lasso.best_estimator_['lasso'].coef_)}\r\n",
    "dict(sorted(lasso_weights.items(), key=lambda item: item[1], reverse=True))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Ba': 12.747910288530562,\n",
       " 'wtd_gmean_ThermalConductivity': 12.364521222651708,\n",
       " 'wtd_mean_ThermalConductivity': 11.947603500510482,\n",
       " 'range_atomic_mass': 6.955103730407498,\n",
       " 'wtd_std_Valence': 6.5603440594602995,\n",
       " 'wtd_gmean_ElectronAffinity': 4.328328539587897,\n",
       " 'Bi': 3.97490881456744,\n",
       " 'wtd_entropy_atomic_mass': 3.9508095199980304,\n",
       " 'wtd_entropy_ThermalConductivity': 3.924730652055882,\n",
       " 'Ca': 3.3178161344471087,\n",
       " 'wtd_entropy_FusionHeat': 2.8062532004771885,\n",
       " 'wtd_entropy_ElectronAffinity': 2.5435733987843343,\n",
       " 'mean_Density': 1.7386569847400537,\n",
       " 'Si': 1.6972961033719114,\n",
       " 'mean_fie': 1.5836528543493764,\n",
       " 'Sr': 1.3418330269172292,\n",
       " 'wtd_range_atomic_mass': 1.2028148462719683,\n",
       " 'range_atomic_radius': 1.1880683928528302,\n",
       " 'As': 1.1522532857677543,\n",
       " 'S': 1.0526391079045863,\n",
       " 'wtd_std_FusionHeat': 1.046997414247526,\n",
       " 'mean_ThermalConductivity': 0.9839999275894176,\n",
       " 'wtd_std_atomic_mass': 0.7363630703449164,\n",
       " 'wtd_range_FusionHeat': 0.5820119169251454,\n",
       " 'wtd_range_Density': 0.5578557645139761,\n",
       " 'Ge': 0.5532431718577415,\n",
       " 'std_ElectronAffinity': 0.419474827926435,\n",
       " 'B': 0.3381522940299983,\n",
       " 'Al': 0.32446796220072877,\n",
       " 'wtd_std_Density': 0.2711147428607582,\n",
       " 'Ti': 0.16585668759745884,\n",
       " 'Tl': 0.16003493811249433,\n",
       " 'Nb': 0.1480172271062835,\n",
       " 'C': 0.12906196232215852,\n",
       " 'V': 0.12640116643325944,\n",
       " 'La': 0.12478981428672298,\n",
       " 'Zr': 0.05164513864755047,\n",
       " 'Pd': 0.03141891595180431,\n",
       " 'Mo': 0.021814294515427206,\n",
       " 'number_of_elements': 0.0,\n",
       " 'mean_atomic_mass': 0.0,\n",
       " 'wtd_mean_atomic_mass': 0.0,\n",
       " 'gmean_atomic_mass': 0.0,\n",
       " 'wtd_gmean_atomic_mass': 0.0,\n",
       " 'entropy_atomic_mass': 0.0,\n",
       " 'std_atomic_mass': 0.0,\n",
       " 'wtd_mean_fie': 0.0,\n",
       " 'gmean_fie': 0.0,\n",
       " 'wtd_gmean_fie': 0.0,\n",
       " 'entropy_fie': 0.0,\n",
       " 'wtd_entropy_fie': 0.0,\n",
       " 'range_fie': 0.0,\n",
       " 'wtd_range_fie': 0.0,\n",
       " 'std_fie': 0.0,\n",
       " 'wtd_std_fie': 0.0,\n",
       " 'mean_atomic_radius': 0.0,\n",
       " 'wtd_mean_atomic_radius': 0.0,\n",
       " 'gmean_atomic_radius': 0.0,\n",
       " 'wtd_gmean_atomic_radius': 0.0,\n",
       " 'entropy_atomic_radius': 0.0,\n",
       " 'wtd_entropy_atomic_radius': 0.0,\n",
       " 'wtd_range_atomic_radius': 0.0,\n",
       " 'std_atomic_radius': 0.0,\n",
       " 'wtd_std_atomic_radius': 0.0,\n",
       " 'wtd_mean_Density': 0.0,\n",
       " 'gmean_Density': 0.0,\n",
       " 'wtd_gmean_Density': 0.0,\n",
       " 'entropy_Density': 0.0,\n",
       " 'wtd_entropy_Density': 0.0,\n",
       " 'range_Density': 0.0,\n",
       " 'std_Density': 0.0,\n",
       " 'mean_ElectronAffinity': 0.0,\n",
       " 'wtd_mean_ElectronAffinity': 0.0,\n",
       " 'gmean_ElectronAffinity': 0.0,\n",
       " 'entropy_ElectronAffinity': 0.0,\n",
       " 'range_ElectronAffinity': 0.0,\n",
       " 'wtd_range_ElectronAffinity': 0.0,\n",
       " 'wtd_std_ElectronAffinity': 0.0,\n",
       " 'mean_FusionHeat': 0.0,\n",
       " 'wtd_mean_FusionHeat': 0.0,\n",
       " 'gmean_FusionHeat': 0.0,\n",
       " 'wtd_gmean_FusionHeat': 0.0,\n",
       " 'entropy_FusionHeat': 0.0,\n",
       " 'range_FusionHeat': 0.0,\n",
       " 'std_FusionHeat': 0.0,\n",
       " 'gmean_ThermalConductivity': 0.0,\n",
       " 'entropy_ThermalConductivity': 0.0,\n",
       " 'range_ThermalConductivity': 0.0,\n",
       " 'wtd_range_ThermalConductivity': 0.0,\n",
       " 'std_ThermalConductivity': 0.0,\n",
       " 'wtd_std_ThermalConductivity': 0.0,\n",
       " 'mean_Valence': 0.0,\n",
       " 'wtd_mean_Valence': 0.0,\n",
       " 'gmean_Valence': 0.0,\n",
       " 'wtd_gmean_Valence': 0.0,\n",
       " 'entropy_Valence': 0.0,\n",
       " 'wtd_entropy_Valence': 0.0,\n",
       " 'range_Valence': 0.0,\n",
       " 'wtd_range_Valence': 0.0,\n",
       " 'std_Valence': 0.0,\n",
       " 'H': 0.0,\n",
       " 'Li': 0.0,\n",
       " 'Be': 0.0,\n",
       " 'N': 0.0,\n",
       " 'O': 0.0,\n",
       " 'F': 0.0,\n",
       " 'Na': 0.0,\n",
       " 'Mg': 0.0,\n",
       " 'P': 0.0,\n",
       " 'Cl': 0.0,\n",
       " 'K': 0.0,\n",
       " 'Sc': 0.0,\n",
       " 'Cr': 0.0,\n",
       " 'Mn': 0.0,\n",
       " 'Fe': 0.0,\n",
       " 'Co': 0.0,\n",
       " 'Ni': 0.0,\n",
       " 'Cu': 0.0,\n",
       " 'Zn': 0.0,\n",
       " 'Ga': 0.0,\n",
       " 'Se': 0.0,\n",
       " 'Br': 0.0,\n",
       " 'Rb': 0.0,\n",
       " 'Y': 0.0,\n",
       " 'Tc': 0.0,\n",
       " 'Ru': 0.0,\n",
       " 'Rh': 0.0,\n",
       " 'Ag': 0.0,\n",
       " 'Cd': 0.0,\n",
       " 'In': 0.0,\n",
       " 'Sn': 0.0,\n",
       " 'Sb': 0.0,\n",
       " 'Te': 0.0,\n",
       " 'I': 0.0,\n",
       " 'Cs': 0.0,\n",
       " 'Ce': 0.0,\n",
       " 'Pr': 0.0,\n",
       " 'Nd': 0.0,\n",
       " 'Sm': 0.0,\n",
       " 'Eu': 0.0,\n",
       " 'Gd': 0.0,\n",
       " 'Tb': 0.0,\n",
       " 'Dy': 0.0,\n",
       " 'Ho': 0.0,\n",
       " 'Er': 0.0,\n",
       " 'Tm': 0.0,\n",
       " 'Yb': 0.0,\n",
       " 'Lu': 0.0,\n",
       " 'Hf': 0.0,\n",
       " 'Ta': 0.0,\n",
       " 'W': 0.0,\n",
       " 'Re': 0.0,\n",
       " 'Os': 0.0,\n",
       " 'Ir': 0.0,\n",
       " 'Pt': 0.0,\n",
       " 'Au': 0.0,\n",
       " 'Hg': 0.0,\n",
       " 'Pb': 0.0}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "ridge_weights = {data.columns[k]:abs(v) for k, v in enumerate(gs_ridge.best_estimator_['ridge'].coef_)}\r\n",
    "dict(sorted(ridge_weights.items(), key=lambda item: item[1], reverse=True))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Ba': 10.370112488683707,\n",
       " 'wtd_mean_ThermalConductivity': 6.483779782189918,\n",
       " 'wtd_std_Valence': 6.0513734789108,\n",
       " 'wtd_gmean_ThermalConductivity': 5.803254805998128,\n",
       " 'range_atomic_mass': 4.462011897968567,\n",
       " 'Bi': 4.206544355861212,\n",
       " 'wtd_std_ThermalConductivity': 3.828515214405895,\n",
       " 'wtd_gmean_ElectronAffinity': 3.5891168955214394,\n",
       " 'Ca': 3.410551989821315,\n",
       " 'wtd_entropy_ElectronAffinity': 3.394887646825798,\n",
       " 'wtd_std_atomic_mass': 3.196081179425659,\n",
       " 'wtd_entropy_atomic_mass': 2.8317915478141638,\n",
       " 'mean_ThermalConductivity': 2.7609890505040484,\n",
       " 'wtd_entropy_FusionHeat': 2.5880223369523963,\n",
       " 'gmean_ThermalConductivity': 2.5244734431093048,\n",
       " 'range_atomic_radius': 2.52409341126252,\n",
       " 'Hg': 2.5220927025032776,\n",
       " 'wtd_range_ElectronAffinity': 2.521911168615366,\n",
       " 'std_ElectronAffinity': 2.4642131470561,\n",
       " 'Tl': 2.454638635844284,\n",
       " 'Ag': 2.3027845073780266,\n",
       " 'std_atomic_mass': 2.274435755236311,\n",
       " 'wtd_mean_atomic_radius': 2.2379734200628874,\n",
       " 'wtd_std_atomic_radius': 2.225423834408331,\n",
       " 'number_of_elements': 2.122047583186745,\n",
       " 'wtd_range_atomic_mass': 2.0528071077964136,\n",
       " 'range_ElectronAffinity': 2.0053606678417855,\n",
       " 'wtd_entropy_ThermalConductivity': 2.000837021011655,\n",
       " 'Cl': 1.9297673269011075,\n",
       " 'wtd_entropy_atomic_radius': 1.9121479085863473,\n",
       " 'Ce': 1.8901282417445275,\n",
       " 'wtd_std_Density': 1.8754816171782929,\n",
       " 'entropy_ThermalConductivity': 1.868334488881393,\n",
       " 'As': 1.841577308169385,\n",
       " 'Nd': 1.8397797864841345,\n",
       " 'wtd_range_Density': 1.76051799078602,\n",
       " 'mean_Density': 1.7353459958360702,\n",
       " 'wtd_entropy_fie': 1.7224005390501085,\n",
       " 'wtd_range_Valence': 1.6559624987557906,\n",
       " 'gmean_Density': 1.5920937181678754,\n",
       " 'wtd_std_FusionHeat': 1.5733640928211254,\n",
       " 'Si': 1.5648684463522984,\n",
       " 'wtd_range_atomic_radius': 1.560302361819028,\n",
       " 'range_ThermalConductivity': 1.5006385187436602,\n",
       " 'Eu': 1.4879310575091205,\n",
       " 'entropy_atomic_mass': 1.4004485747790711,\n",
       " 'range_fie': 1.3949199273322916,\n",
       " 'mean_atomic_mass': 1.3902674557297248,\n",
       " 'wtd_mean_atomic_mass': 1.374116100908515,\n",
       " 'std_ThermalConductivity': 1.3479725189697243,\n",
       " 'B': 1.267638502967452,\n",
       " 'Y': 1.2673991509755147,\n",
       " 'mean_FusionHeat': 1.2397777333138973,\n",
       " 'entropy_ElectronAffinity': 1.228590658016926,\n",
       " 'mean_ElectronAffinity': 1.1613942978971574,\n",
       " 'N': 1.1523885336052448,\n",
       " 'gmean_atomic_radius': 1.0789354008968335,\n",
       " 'Lu': 1.0502294812614594,\n",
       " 'S': 1.0185770550728528,\n",
       " 'wtd_entropy_Valence': 0.9689201386204973,\n",
       " 'Mg': 0.9488433421154072,\n",
       " 'wtd_range_FusionHeat': 0.9288042033723658,\n",
       " 'wtd_std_ElectronAffinity': 0.9243360421323548,\n",
       " 'gmean_fie': 0.8945877730999212,\n",
       " 'entropy_Density': 0.8838734917344984,\n",
       " 'Ge': 0.8828450746666208,\n",
       " 'std_fie': 0.8737810087448808,\n",
       " 'std_Valence': 0.8431055330812637,\n",
       " 'Hf': 0.7984913127493142,\n",
       " 'Li': 0.796634524993234,\n",
       " 'O': 0.7911594370653655,\n",
       " 'entropy_fie': 0.7670593506228472,\n",
       " 'Co': 0.7484383138135032,\n",
       " 'Pt': 0.7179778930779598,\n",
       " 'range_FusionHeat': 0.6993648955432503,\n",
       " 'Gd': 0.6783795504250749,\n",
       " 'Zn': 0.6659617240307146,\n",
       " 'std_atomic_radius': 0.6456580872403753,\n",
       " 'wtd_mean_Density': 0.6394065002973661,\n",
       " 'wtd_gmean_atomic_radius': 0.6349529219528366,\n",
       " 'range_Valence': 0.6296974141258311,\n",
       " 'Al': 0.6143174974849577,\n",
       " 'P': 0.6104434335937698,\n",
       " 'Yb': 0.6080235920245325,\n",
       " 'wtd_mean_Valence': 0.593962899908165,\n",
       " 'Be': 0.592345397108514,\n",
       " 'Sr': 0.5875302542168738,\n",
       " 'std_FusionHeat': 0.58598022598591,\n",
       " 'Os': 0.5725433309843191,\n",
       " 'Pb': 0.5626411211616261,\n",
       " 'wtd_entropy_Density': 0.5364630842177518,\n",
       " 'entropy_Valence': 0.5236726357001019,\n",
       " 'Cu': 0.4991546792018265,\n",
       " 'mean_fie': 0.4918038098409988,\n",
       " 'Rb': 0.4773993180614367,\n",
       " 'entropy_atomic_radius': 0.4767926026980457,\n",
       " 'range_Density': 0.46736777500558535,\n",
       " 'K': 0.45837800225386877,\n",
       " 'I': 0.43450304821194285,\n",
       " 'gmean_ElectronAffinity': 0.43211638269764685,\n",
       " 'Er': 0.42785929791818406,\n",
       " 'mean_Valence': 0.4241711056344406,\n",
       " 'Ni': 0.421455224947681,\n",
       " 'wtd_std_fie': 0.4066037516336181,\n",
       " 'std_Density': 0.35452227779509865,\n",
       " 'Se': 0.3414538426893445,\n",
       " 'gmean_Valence': 0.3230833254656482,\n",
       " 'F': 0.3208537195673984,\n",
       " 'H': 0.29538031696653255,\n",
       " 'mean_atomic_radius': 0.2807679308097941,\n",
       " 'Tc': 0.2712961013666125,\n",
       " 'Dy': 0.26367612222767717,\n",
       " 'wtd_gmean_FusionHeat': 0.25240899697080105,\n",
       " 'gmean_atomic_mass': 0.2514192058843516,\n",
       " 'Mn': 0.2502606972919306,\n",
       " 'W': 0.21782865620125683,\n",
       " 'Pd': 0.21323581881772546,\n",
       " 'wtd_gmean_Density': 0.20869211754232894,\n",
       " 'Ho': 0.2043372529766807,\n",
       " 'wtd_mean_fie': 0.19818411312443474,\n",
       " 'Au': 0.19735543241464792,\n",
       " 'Ti': 0.192919562062863,\n",
       " 'wtd_gmean_fie': 0.19234590189022066,\n",
       " 'Pr': 0.1670285897712909,\n",
       " 'wtd_gmean_Valence': 0.16432341792085112,\n",
       " 'Tb': 0.1594766931104036,\n",
       " 'La': 0.15544066245255522,\n",
       " 'Cd': 0.1550548446835377,\n",
       " 'Nb': 0.13505317534318065,\n",
       " 'V': 0.11956580487924762,\n",
       " 'Tm': 0.11811228993226197,\n",
       " 'Rh': 0.10230559755799028,\n",
       " 'Br': 0.09999983242006538,\n",
       " 'wtd_mean_ElectronAffinity': 0.09068023240866165,\n",
       " 'wtd_range_fie': 0.0903801142805733,\n",
       " 'entropy_FusionHeat': 0.08893691361159375,\n",
       " 'Ta': 0.08348592631428255,\n",
       " 'Sm': 0.0829435316009302,\n",
       " 'wtd_mean_FusionHeat': 0.08217011496939779,\n",
       " 'Cs': 0.07812043275290355,\n",
       " 'Fe': 0.06758399352255462,\n",
       " 'Sn': 0.0614433197300692,\n",
       " 'wtd_range_ThermalConductivity': 0.061299030651588204,\n",
       " 'Ru': 0.04617699829343605,\n",
       " 'Ga': 0.039355760232121506,\n",
       " 'Zr': 0.03887823108024104,\n",
       " 'In': 0.03319234984059206,\n",
       " 'Te': 0.03318108200549427,\n",
       " 'Cr': 0.02691993043988254,\n",
       " 'Sb': 0.024082020339040115,\n",
       " 'C': 0.021407473314099756,\n",
       " 'Sc': 0.020594831716504336,\n",
       " 'Na': 0.016942450459675514,\n",
       " 'Ir': 0.01337156203657521,\n",
       " 'wtd_gmean_atomic_mass': 0.013143277576503206,\n",
       " 'Re': 0.011506132232889477,\n",
       " 'Mo': 0.004963541124364872,\n",
       " 'gmean_FusionHeat': 0.002024485315775404}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Interpretability & Explainability\r\n",
    "Using at least one of your models above (if multiple were trained):\r\n",
    "\r\n",
    "- Which variable(s) was (were) \"\"most important\"\" and why?  How did you come to the conclusion and how should your audience interpret this?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Case Conclusions\r\n",
    "After all of your technical analysis and modeling; what are you proposing to your audience and why?  How should they view your results and what should they consider when moving forward?  Are there other approaches you'd recommend exploring?  This is where you \"bring it all home\" in language they understand."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eaa798b471aa1a0109429a408b0faab53065248ef7f5a5989f90756771672ef6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('NLP': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}