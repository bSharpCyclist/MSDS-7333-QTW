{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSDS 7331 - Case Study 3 - Clasification of e-mail as ham or spam\n",
    "Daniel Crouthamel\n",
    "\n",
    "Sophia Wu\n",
    "\n",
    "Fabio Savorgnan\n",
    "\n",
    "Bo Yun\n",
    "\n",
    "# Introduction\n",
    "\n",
    "In this study, we will be building a classifier to predict busines that will go on bankrupcy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always state the objective at the beginning of every case (a guideline you should follow in real life as well) and provide some initial \"Business Understanding\" statements (i.e., what is trying to be solved for and why might it be important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries and reading in file\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "#general sklearn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Yellowbrick\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "\n",
    "#Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Files\n",
    "from os import listdir, getcwd, chdir\n",
    "from os.path import isfile, join, dirname, realpath\n",
    "from scipy.io import arff\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data engeniering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the data being used in the case using appropriate mediums (charts, graphs, tables); address questions such as: Are there missing values? Which variables are needed (which ones are not)? What assumptions or conclusions are you drawing that need to be relayed to your audience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['data/1year.arff', 'data/2year.arff', 'data/3year.arff', 'data/4year.arff', 'data/5year.arff']\n",
    "\n",
    "\n",
    "df = pd.DataFrame(arff.loadarff(files[0])[0])\n",
    "\n",
    "for f in files[1:]:\n",
    "    data_temp = arff.loadarff(f)\n",
    "    df_temp = pd.DataFrame(data_temp[0])\n",
    "    print(df_temp.shape)\n",
    "    df = df.merge(df_temp,how='outer') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values \n",
    "nan_columns = []\n",
    "nan_values = []\n",
    "\n",
    "for column in df.columns:\n",
    "    nan_columns.append(column)\n",
    "    nan_values.append(df[column].isnull().sum())\n",
    "    \n",
    "nan_dict = {'Attributes': nan_columns, \"Nan Count\": nan_values}\n",
    "nan_df = pd.DataFrame(nan_dict)\n",
    "\n",
    "ax = nan_df.plot(kind='barh', stacked=True, figsize=(20, 20), rot=0, xlabel='Attribute', ylabel='Count', title = 'NANS')\n",
    "for c in ax.containers:\n",
    "    ax.bar_label(c, label_type='edge', fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fill all missing values with the mean of the column \n",
    "df = df.where(pd.notna(df), df.interpolate(), axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After filling th missing values, check it again.\n",
    "missing = df.isnull().any(axis=1).sum()\n",
    "len_before = df.shape[0]\n",
    "print(f\"Total records missing data: {missing}\\n\"\n",
    "      f\"Total percent of incomplete records: {missing/len_before*100:.2f}%\"\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and transform to binary 0 0r 1 the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"class\"].unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This show that the target is very inbalanced\n",
    "df['class'].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to better show the inbalanced target\n",
    "plt.hist(df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the target in 0 an1\n",
    "\n",
    "# classes = []\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     class_val = row['class']\n",
    "#     if class_val not in classes:\n",
    "#         classes.append(class_val)\n",
    "\n",
    "# class_dict = {}\n",
    "\n",
    "# for index, i in enumerate(classes):\n",
    "#     class_dict.update({i:str(index)})\n",
    "    \n",
    "# df['class'] = df['class'].map(class_dict)\n",
    "\n",
    "# All of the code above can be replaced with this\n",
    "df['class'] = df['class'].replace([b'0',b'1'],[0,1])\n",
    "\n",
    "df['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Check all correlations\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True) # one of the many color mappings\n",
    "\n",
    "# show the heatmap\n",
    "sns.set(style=\"darkgrid\") # one of the many styles to plot using\n",
    "f, ax = plt.subplots(figsize=(18, 9))\n",
    "chart=sns.heatmap(df.corr(), cmap=cmap, annot=False)\n",
    "chart.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List all the top correlations\n",
    "sort_corr = pd.DataFrame(abs(df.corr().unstack().sort_values().drop_duplicates()))\n",
    "sort_corr.rename(columns={0:'Top Abs Corr'}, inplace=True)\n",
    "sort_corr.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the differents columns of the data with pandas profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "profile = ProfileReport(df, minimal=True)\n",
    "profile.to_file(output_file=\"output.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided that missing values less than 10 % we bould keep as variables for our model because we would impute with the meam for the missing values. We will use the simple imputer.\n",
    "\n",
    "So we decided to take out the Attr 21  \"Sales (n) / sales (n-1)\" because it has 13.5 missing values, also we decide to take out Attr 37 \"Profit on operating activities / financial expenses\" because it has 43.7 missing values. We believe specially for Attr 37 that this amount of missing values would not be able to replace in a meaninfull way with imputation.\n",
    "\n",
    "We also plan to normalize the data using the rubost scaler.\n",
    "\n",
    "Please see the attached pandas profiles.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales (n) / sales (n-1)\n",
    "plt.hist(df[\"Attr21\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profit on operating activities / financial expenses\n",
    "plt.hist(df[\"Attr37\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataframe\n",
    "df= df.drop([\"Attr21\"], axis = 1)\n",
    "df= df.drop([\"Attr37\"], axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which methods are you proposing to utilize to solve the problem?  Why is this method appropriate given the business objective? How will you determine if your approach is useful (or how will you differentiate which approach is more useful than another)?  More specifically, what evaluation metrics are most useful given that the problem is a binary-classification one (ex., Accuracy, F1-score, Precision, Recall, AUC, etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randon Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test and train data\n",
    "\n",
    "X = df.loc[:, df.columns != 'class'].values\n",
    "y = df['class'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute\n",
    "\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_mean.fit(X_train)\n",
    "X_train = imp_mean.transform(X_train)\n",
    "X_test = imp_mean.transform(X_test)\n",
    "\n",
    "# Normalize the data\n",
    "transformer = RobustScaler().fit(X_train)\n",
    "transformer = RobustScaler().fit(X_test)\n",
    "X_train = transformer.transform(X_train)\n",
    "X_test= transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, your primary task is to build both a Random Forest and XGBoost model to accurately predict bankruptcy and will involve the following steps:\n",
    "\n",
    "- Specify your sampling methodology\n",
    "- Setup your models - highlighting any important parameters\n",
    "- Analyze each model's performance - referencing your chosen evaluation metric (including supplemental visuals and analysis where appropriate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Grid Search for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier\n",
    "clf = RandomForestClassifier(n_estimators=20)\n",
    "\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)\n",
    "\n",
    "# use a full grid over all parameters\n",
    "param_grid = {\"max_depth\": [3, None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
    "start = time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
    "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
    "report(grid_search.cv_results_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_rf_train = grid_search.predict(X_train)\n",
    "accuracy_score(y_hat_rf_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix train\n",
    "confusion_matrix(y_train, y_hat_rf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_rf_test = grid_search.predict(X_test)\n",
    "accuracy_score(y_hat_rf_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix test\n",
    "confusion_matrix(y_test, y_hat_rf_test)\n",
    "disp = ConfusionMatrixDisplay.from_estimator(grid_search, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision and recall\n",
    "print(\"Recall:\", recall_score(y_test, y_hat_rf_test, pos_label=\"1\", average='binary'))\n",
    "print(\"Precision:\", precision_score(y_test, y_hat_rf_test, pos_label=\"1\", average='binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_precision_recall_curve(grid_search, X_test, y_test,)\n",
    "disp.ax_.set_title('Precision-Recall Curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Disp = RocCurveDisplay.from_estimator(grid_search, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate= 0.1,\n",
    "    max_depth=10, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model for comparison to random forest, in order to see if we can improve the Random forest best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_G = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision and recall\n",
    "print(\"Recall:\", recall_score(y_test, y_hat_G, pos_label=\"1\", average='binary'))\n",
    "print(\"Precision:\", precision_score(y_test, y_hat_G, pos_label=\"1\", average='binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_precision_recall_curve(clf, X_test, y_test,)\n",
    "disp.ax_.set_title('Precision-Recall Curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion_matrix(y_test, y_hat_G)\n",
    "disp = ConfusionMatrixDisplay.from_estimator(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isp = RocCurveDisplay.from_estimator(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the grid search for GBoost in order to see if there is any improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {'n_estimators':range(100, 500, 1000),\n",
    "'max_depth':range(10,16), 'min_samples_split':range(1, 10)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1,min_samples_leaf= 1,random_state= 0), \n",
    "param_grid = param_test1,n_jobs=4).fit(X_train, y_train)\n",
    "gsearch1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_GG = gsearch1.predict(X_test)\n",
    "# Precision and recall\n",
    "print(\"Recall:\", recall_score(y_test, y_hat_GG, pos_label=\"1\", average='binary'))\n",
    "print(\"Precision:\", precision_score(y_test, y_hat_GG, pos_label=\"1\", average='binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Interpretability & Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using at least one of your models above (if multiple were trained):\n",
    "\n",
    "- Which variable(s) was (were) \"most important\" and why?  How did you come to the conclusion and how should your audience interpret this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yellow brick\n",
    "\n",
    "# Set xyz to the appropriate variable\n",
    "viz = FeatureImportances(xyz, topn=6, relative=False)\n",
    "viz.fit(X_test, y_test)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of your technical analysis and modeling; what are you proposing to your audience and why?  How should they view your results and what should they consider when moving forward?  Are there other approaches you'd recommend exploring?  This is where you \"bring it all home\" in language they understand."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34df73abb4db1cfa3ba52df322ae071479434ee8a073529484db6547456101f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('ML7331': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
