{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSDS 7331 - Case Study 1 - Superconducting Materials\r\n",
    "Daniel Crouthamel\r\n",
    "Sophia Wu\r\n",
    "Fabio Savorgnan\r\n",
    "Bo Yun\r\n",
    "\r\n",
    "## Business Understanding\r\n",
    "--\r\n",
    "You should always state the objective at the beginning of every case (a guideline you should follow in real life as well) and provide some initial \"Business Understanding\" statements (i.e., what is trying to be solved for and why might it be important)\r\n",
    "--\r\n",
    "\r\n",
    "**Objective:** \r\n",
    "The objective of this case study is to explore Linerar Regression with L1 and L2 regularization, and the impact to predicting the critical temperature of a superconductor. Additionally, feature importance is also investigated with the best model. Three models will be considered.\r\n",
    "\r\n",
    "* Lasso (L1)\r\n",
    "* Ridge (L2)\r\n",
    "* Elastic Net (L1 and L2).\r\n",
    "\r\n",
    "**Todo!!**\r\n",
    "Summarize/background on L1/L2, the affect of alpha, l1 ratio, the hyperparameters.\r\n",
    "\r\n",
    "Grid search will be performed on each model to find the optimal hyperparameters. We'll then use the optimal hyperparameters for each model and then determine which model performs the best. Along the way will explore the importance of normalization and scaling and condlude with a summary of the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Evaluation and Engineering\r\n",
    "--\r\n",
    "Summarize the data being used in the case using appropriate mediums (charts, graphs, tables); address questions such as: Are there missing values? Which variables are needed (which ones are not)? What assumptions or conclusions are you drawing that need to be relayed to your audience?\r\n",
    "--\r\n",
    "\r\n",
    "Below we'll load our supoerconductor data and then perform a quick data exploration. Our data consists of two files which were merged together. Some initial obervations are:\r\n",
    "\r\n",
    "* There is no missing data\r\n",
    "* We have features with constant values\r\n",
    "* There is one string object in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21263 entries, 0 to 21262\n",
      "Columns: 169 entries, number_of_elements to material\n",
      "dtypes: float64(156), int64(12), object(1)\n",
      "memory usage: 27.4+ MB\n",
      "\n",
      "Missing Data? Index([], dtype='object')\n",
      "\n",
      "Columns that have the same value ['He' 'Ne' 'Ar' 'Kr' 'Xe' 'Pm' 'Po' 'At' 'Rn']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "# Load Data\r\n",
    "data_train = pd.read_csv('./data/train.csv')\r\n",
    "data_materials = pd.read_csv('./data/unique_m.csv')\r\n",
    "\r\n",
    "# Drop the duplicate column 'critical_temp' in the first frame\r\n",
    "data_train = data_train.drop(['critical_temp'], axis=1)\r\n",
    "\r\n",
    "# Merge the two frames\r\n",
    "data = pd.merge(data_train, data_materials, left_index=True, right_index=True)\r\n",
    "\r\n",
    "# Data frame to csv drop index\r\n",
    "data.to_csv('./data/super_conducter_data.csv', index=False)\r\n",
    "\r\n",
    "# Print out some typicaly descriptive statistics\r\n",
    "print(\"\")\r\n",
    "data.info()\r\n",
    "data.describe()\r\n",
    "\r\n",
    "# Do we have missing data?\r\n",
    "print(\"\")\r\n",
    "print(\"Missing Data?\", data.columns[data.isnull().any().values])\r\n",
    "\r\n",
    "# Columns with a Constant value\r\n",
    "print(\"\")\r\n",
    "print(\"Columns that have the same value\", data.columns[data.nunique() <= 1].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string feature will be removed from our data set. This appears to be a name, and some values are duplicated. We felt it was save to remove. Addtionally, the features identified above with constant values will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21263, 159)\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with constant values\r\n",
    "data.drop(columns=['material', 'He', 'Ne', 'Ar', 'Kr', 'Xe', 'Pm', 'Po', 'At', 'Rn'], inplace=True)\r\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is used to create a pandas profile, which is an easy way to see summary statistics for each feature. The html file will be provided as part of the case study submission. Some findings from that report are:\r\n",
    "\r\n",
    "* Confirms no missing values\r\n",
    "* There are no negative values\r\n",
    "* Varying distrubutions for the features, some bell shaped, others poisson, etc.\r\n",
    "* We have features with skewness and outliers.\r\n",
    "\r\n",
    "To run the code, uncomment the last 3 lines of code below and run the cell. The first two lines indicate what needs to be installed. You will need to use an older version of pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## install pandas 1.2.4\r\n",
    "## pip install pandas-profiling==2.8.0\r\n",
    "\r\n",
    "# from pandas_profiling import ProfileReport\r\n",
    "# profile = ProfileReport(data, title=\"Pandas Profiling Report\", minimal=True)\r\n",
    "# profile.to_file(output_file=\"PandasProfile.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Preparations\r\n",
    "--\r\n",
    "Which methods are you proposing to utilize to solve the problem?  Why is this method appropriate given the business objective? How will you determine if your approach is useful (or how will you differentiate which approach is more useful than another)?  More specifically, what evaluation metrics are most useful given that the problem is a regression one (ex., RMSE, logloss, MAE, etc.)?\r\n",
    "--\r\n",
    "\r\n",
    "As mentioned above, we'll be using grid search on three different models. For each model we'll use R2 as a cross validation metric, which can can explain how much of the variance is capatured by the model. We'll then use MAE (Mean Absolute Error) to differentiate between if Lasso, Ridge, or Elastic Net performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building and Evaluation\r\n",
    "--\r\n",
    "In this case, your primary task is to build a linear regression model using L1 or L2 regularization (or both) to predict the critical temperature and will involve the following steps:\r\n",
    "\r\n",
    "- Specify your sampling methodology\r\n",
    "- Setup your model(s) - specifying the regularization type chosen and including the parameters utilized by the model\r\n",
    "- Analyze your model's performance - referencing your chosen evaluation metric (including supplemental visuals and analysis where appropriate)\r\n",
    "--\r\n",
    "\r\n",
    "The code below creates 3 models and performs grid search on each model. For Lasso and Ridge, we'll vary the hyperparameter alpha. For Elastic Net, we'll vary the hyperparameters alpha and l1_ratio. Ranges are shown below in the code.\r\n",
    "\r\n",
    "Normalization and scaling are performed on the data. There are several different ones that can be used, but in the end we decided to go with RobustScaler, which scales features using statistics that are robust to outliers.\r\n",
    "\r\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html\r\n",
    "\r\n",
    "We additionally use a Pipeline to define the order of steps. By using a Pipeline with GridSearchCV we only scale the data in the training set. The test sets in each fold are then scaled using the same scaler.\r\n",
    "\r\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html\r\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\r\n",
    "https://stats.stackexchange.com/questions/445259/combining-pca-feature-scaling-and-cross-validation-without-training-test-data\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso\n",
      "0.7129520975572088\n",
      "{'lasso__alpha': 0.3}\n",
      "\n",
      "Ridge\n",
      "0.7065312755243309\n",
      "{'ridge__alpha': 1000}\n",
      "\n",
      "Elastic\n",
      "0.7083853891920866\n",
      "{'elasticnet__alpha': 0.3, 'elasticnet__l1_ratio': 0.9}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.preprocessing import RobustScaler\r\n",
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "from sklearn.linear_model import Lasso\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "from sklearn.linear_model import ElasticNet\r\n",
    "from sklearn.linear_model import Ridge\r\n",
    "from sklearn.pipeline import make_pipeline\r\n",
    "\r\n",
    "X = data.drop(columns=['critical_temp']).copy(deep=True)\r\n",
    "y = data.loc[:,'critical_temp'].copy(deep=True)\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test =\\\r\n",
    "    train_test_split(X, y,\r\n",
    "    test_size=0.2,\r\n",
    "    random_state=1)\r\n",
    "\r\n",
    "lasso_pipe_svc = make_pipeline(RobustScaler(), Lasso(random_state=1))\r\n",
    "ridge_pipe_svc = make_pipeline(RobustScaler(), Ridge(random_state=1))\r\n",
    "elastic_pipe_svc = make_pipeline(RobustScaler(), ElasticNet(random_state=1))\r\n",
    "\r\n",
    "param_range = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 10, 100, 1000, 10000]\r\n",
    "param_l1_ratio = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\r\n",
    "\r\n",
    "param_grid_lasso = [{'lasso__alpha': param_range}]\r\n",
    "param_grid_ridge = [{'ridge__alpha': param_range}]\r\n",
    "param_grid_elastic = [{'elasticnet__alpha': param_range, 'elasticnet__l1_ratio': param_l1_ratio}]\r\n",
    "\r\n",
    "gs_lasso = GridSearchCV(estimator=lasso_pipe_svc, param_grid=param_grid_lasso, scoring='r2', cv=5, n_jobs=-1)\r\n",
    "gs_lasso.fit(X_train, y_train)\r\n",
    "\r\n",
    "gs_ridge = GridSearchCV(estimator=ridge_pipe_svc, param_grid=param_grid_ridge, scoring='r2', cv=5, n_jobs=-1)\r\n",
    "gs_ridge.fit(X_train, y_train)\r\n",
    "\r\n",
    "gs_elastic = GridSearchCV(estimator=elastic_pipe_svc, param_grid=param_grid_elastic, scoring='r2', cv=5, n_jobs=-1)\r\n",
    "gs_elastic.fit(X_train, y_train)\r\n",
    "\r\n",
    "print(\"Lasso\")\r\n",
    "print(gs_lasso.best_score_)\r\n",
    "print(gs_lasso.best_params_)\r\n",
    "print(\"\")\r\n",
    "\r\n",
    "print(\"Ridge\")\r\n",
    "print(gs_ridge.best_score_)\r\n",
    "print(gs_ridge.best_params_)\r\n",
    "print(\"\")\r\n",
    "\r\n",
    "print(\"Elastic\")\r\n",
    "print(gs_elastic.best_score_)\r\n",
    "print(gs_elastic.best_params_)\r\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see that each model performs almost identically on the training data. Recall that L1 is used for Lasso, and L2 is used for Ridget. Elastic Net uses both. The optimal l1_ration hyperparameter found for ElasticNet is 0.9. It's moving into the direction of Lasso. If the ratio is 1, it's Lasso, if it's 0, it's Ridge.\r\n",
    "\r\n",
    "Additionally, the optimal alpha found for Ridge is 1000, which is quite high. It's trying to penalize cofficients harder, which means we'll see more coefficients that are close to zero.\r\n",
    "\r\n",
    "Next we'll perform predictions on our test data. Note that because we are using a pipeline, the scaler is only applied to the test data. This ensures that we have no leakage of information from the training data.\r\n",
    "https://stackoverflow.com/questions/35388647/how-to-use-gridsearchcv-output-for-a-scikit-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso\n",
      "R2 -> 0.7197254814065868\n",
      "MAE -> 13.660714491348584\n",
      "Ridge\n",
      "R2 -> 0.7264906957630274\n",
      "MAE -> 13.359929361601818\n",
      "\n",
      "Elastic\n",
      "R2 -> 0.7146988481193569\n",
      "MAE -> 13.811785860925973\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\r\n",
    "\r\n",
    "# Note the X_test gets run through the pipeline above! Very important, it means that the scaler is also run on the test data\r\n",
    "y_lasso_pred = gs_lasso.predict(X_test)\r\n",
    "y_ridge_pred = gs_ridge.predict(X_test)\r\n",
    "y_elastic_pred = gs_elastic.predict(X_test)\r\n",
    "\r\n",
    "print(\"Lasso\")\r\n",
    "print(\"R2 ->\", metrics.r2_score(y_test, y_lasso_pred))\r\n",
    "print(\"MAE ->\", metrics.mean_absolute_error(y_test, y_lasso_pred))\r\n",
    "\r\n",
    "print(\"Ridge\")\r\n",
    "print(\"R2 ->\", metrics.r2_score(y_test, y_ridge_pred))\r\n",
    "print(\"MAE ->\", metrics.mean_absolute_error(y_test, y_ridge_pred))\r\n",
    "print(\"\")\r\n",
    "\r\n",
    "print(\"Elastic\")\r\n",
    "print(\"R2 ->\", metrics.r2_score(y_test, y_elastic_pred))\r\n",
    "print(\"MAE ->\", metrics.mean_absolute_error(y_test, y_elastic_pred))\r\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see that the R2 values are consistent with what we found on the training data. However, the Ridge model performans marginally better. In this case I'd be inclined to use the Lasso model because of the reduced number of features. Below we output the ABSOLUTE value of each coefficient, in sorted fashion. This helps to give an indication of which features are important. The sign doesn't matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ba': 12.747910288530564,\n",
       " 'wtd_gmean_ThermalConductivity': 12.364521222651712,\n",
       " 'wtd_mean_ThermalConductivity': 11.947603500510484,\n",
       " 'range_atomic_mass': 6.955103730407503,\n",
       " 'wtd_std_Valence': 6.560344059460298,\n",
       " 'wtd_gmean_ElectronAffinity': 4.328328539587896,\n",
       " 'Bi': 3.974908814567439,\n",
       " 'wtd_entropy_atomic_mass': 3.9508095199980278,\n",
       " 'wtd_entropy_ThermalConductivity': 3.924730652055885,\n",
       " 'Ca': 3.3178161344471087,\n",
       " 'wtd_entropy_FusionHeat': 2.806253200477188,\n",
       " 'wtd_entropy_ElectronAffinity': 2.543573398784333,\n",
       " 'mean_Density': 1.738656984740053,\n",
       " 'Si': 1.6972961033719118,\n",
       " 'mean_fie': 1.5836528543493753,\n",
       " 'Sr': 1.3418330269172298,\n",
       " 'wtd_range_atomic_mass': 1.2028148462719708,\n",
       " 'range_atomic_radius': 1.1880683928528337,\n",
       " 'As': 1.1522532857677548,\n",
       " 'S': 1.0526391079045871,\n",
       " 'wtd_std_FusionHeat': 1.0469974142475256,\n",
       " 'mean_ThermalConductivity': 0.9839999275894172,\n",
       " 'wtd_std_atomic_mass': 0.7363630703449185,\n",
       " 'wtd_range_FusionHeat': 0.582011916925144,\n",
       " 'wtd_range_Density': 0.5578557645139776,\n",
       " 'Ge': 0.5532431718577415,\n",
       " 'std_ElectronAffinity': 0.4194748279264357,\n",
       " 'B': 0.3381522940299988,\n",
       " 'Al': 0.32446796220072877,\n",
       " 'wtd_std_Density': 0.2711147428607599,\n",
       " 'Ti': 0.16585668759745875,\n",
       " 'Tl': 0.16003493811249572,\n",
       " 'Nb': 0.14801722710628334,\n",
       " 'C': 0.12906196232215814,\n",
       " 'V': 0.1264011664332592,\n",
       " 'La': 0.12478981428672324,\n",
       " 'Zr': 0.05164513864755051,\n",
       " 'Pd': 0.031418915951804106,\n",
       " 'Mo': 0.021814294515427348,\n",
       " 'number_of_elements': 0.0,\n",
       " 'mean_atomic_mass': 0.0,\n",
       " 'wtd_mean_atomic_mass': 0.0,\n",
       " 'gmean_atomic_mass': 0.0,\n",
       " 'wtd_gmean_atomic_mass': 0.0,\n",
       " 'entropy_atomic_mass': 0.0,\n",
       " 'std_atomic_mass': 0.0,\n",
       " 'wtd_mean_fie': 0.0,\n",
       " 'gmean_fie': 0.0,\n",
       " 'wtd_gmean_fie': 0.0,\n",
       " 'entropy_fie': 0.0,\n",
       " 'wtd_entropy_fie': 0.0,\n",
       " 'range_fie': 0.0,\n",
       " 'wtd_range_fie': 0.0,\n",
       " 'std_fie': 0.0,\n",
       " 'wtd_std_fie': 0.0,\n",
       " 'mean_atomic_radius': 0.0,\n",
       " 'wtd_mean_atomic_radius': 0.0,\n",
       " 'gmean_atomic_radius': 0.0,\n",
       " 'wtd_gmean_atomic_radius': 0.0,\n",
       " 'entropy_atomic_radius': 0.0,\n",
       " 'wtd_entropy_atomic_radius': 0.0,\n",
       " 'wtd_range_atomic_radius': 0.0,\n",
       " 'std_atomic_radius': 0.0,\n",
       " 'wtd_std_atomic_radius': 0.0,\n",
       " 'wtd_mean_Density': 0.0,\n",
       " 'gmean_Density': 0.0,\n",
       " 'wtd_gmean_Density': 0.0,\n",
       " 'entropy_Density': 0.0,\n",
       " 'wtd_entropy_Density': 0.0,\n",
       " 'range_Density': 0.0,\n",
       " 'std_Density': 0.0,\n",
       " 'mean_ElectronAffinity': 0.0,\n",
       " 'wtd_mean_ElectronAffinity': 0.0,\n",
       " 'gmean_ElectronAffinity': 0.0,\n",
       " 'entropy_ElectronAffinity': 0.0,\n",
       " 'range_ElectronAffinity': 0.0,\n",
       " 'wtd_range_ElectronAffinity': 0.0,\n",
       " 'wtd_std_ElectronAffinity': 0.0,\n",
       " 'mean_FusionHeat': 0.0,\n",
       " 'wtd_mean_FusionHeat': 0.0,\n",
       " 'gmean_FusionHeat': 0.0,\n",
       " 'wtd_gmean_FusionHeat': 0.0,\n",
       " 'entropy_FusionHeat': 0.0,\n",
       " 'range_FusionHeat': 0.0,\n",
       " 'std_FusionHeat': 0.0,\n",
       " 'gmean_ThermalConductivity': 0.0,\n",
       " 'entropy_ThermalConductivity': 0.0,\n",
       " 'range_ThermalConductivity': 0.0,\n",
       " 'wtd_range_ThermalConductivity': 0.0,\n",
       " 'std_ThermalConductivity': 0.0,\n",
       " 'wtd_std_ThermalConductivity': 0.0,\n",
       " 'mean_Valence': 0.0,\n",
       " 'wtd_mean_Valence': 0.0,\n",
       " 'gmean_Valence': 0.0,\n",
       " 'wtd_gmean_Valence': 0.0,\n",
       " 'entropy_Valence': 0.0,\n",
       " 'wtd_entropy_Valence': 0.0,\n",
       " 'range_Valence': 0.0,\n",
       " 'wtd_range_Valence': 0.0,\n",
       " 'std_Valence': 0.0,\n",
       " 'H': 0.0,\n",
       " 'Li': 0.0,\n",
       " 'Be': 0.0,\n",
       " 'N': 0.0,\n",
       " 'O': 0.0,\n",
       " 'F': 0.0,\n",
       " 'Na': 0.0,\n",
       " 'Mg': 0.0,\n",
       " 'P': 0.0,\n",
       " 'Cl': 0.0,\n",
       " 'K': 0.0,\n",
       " 'Sc': 0.0,\n",
       " 'Cr': 0.0,\n",
       " 'Mn': 0.0,\n",
       " 'Fe': 0.0,\n",
       " 'Co': 0.0,\n",
       " 'Ni': 0.0,\n",
       " 'Cu': 0.0,\n",
       " 'Zn': 0.0,\n",
       " 'Ga': 0.0,\n",
       " 'Se': 0.0,\n",
       " 'Br': 0.0,\n",
       " 'Rb': 0.0,\n",
       " 'Y': 0.0,\n",
       " 'Tc': 0.0,\n",
       " 'Ru': 0.0,\n",
       " 'Rh': 0.0,\n",
       " 'Ag': 0.0,\n",
       " 'Cd': 0.0,\n",
       " 'In': 0.0,\n",
       " 'Sn': 0.0,\n",
       " 'Sb': 0.0,\n",
       " 'Te': 0.0,\n",
       " 'I': 0.0,\n",
       " 'Cs': 0.0,\n",
       " 'Ce': 0.0,\n",
       " 'Pr': 0.0,\n",
       " 'Nd': 0.0,\n",
       " 'Sm': 0.0,\n",
       " 'Eu': 0.0,\n",
       " 'Gd': 0.0,\n",
       " 'Tb': 0.0,\n",
       " 'Dy': 0.0,\n",
       " 'Ho': 0.0,\n",
       " 'Er': 0.0,\n",
       " 'Tm': 0.0,\n",
       " 'Yb': 0.0,\n",
       " 'Lu': 0.0,\n",
       " 'Hf': 0.0,\n",
       " 'Ta': 0.0,\n",
       " 'W': 0.0,\n",
       " 'Re': 0.0,\n",
       " 'Os': 0.0,\n",
       " 'Ir': 0.0,\n",
       " 'Pt': 0.0,\n",
       " 'Au': 0.0,\n",
       " 'Hg': 0.0,\n",
       " 'Pb': 0.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_weights = {data.columns[k]:abs(v) for k, v in enumerate(gs_lasso.best_estimator_['lasso'].coef_)}\r\n",
    "dict(sorted(lasso_weights.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ba': 10.370112488683777,\n",
       " 'wtd_mean_ThermalConductivity': 6.483779782189887,\n",
       " 'wtd_std_Valence': 6.051373478910801,\n",
       " 'wtd_gmean_ThermalConductivity': 5.803254805998027,\n",
       " 'range_atomic_mass': 4.462011897968528,\n",
       " 'Bi': 4.206544355861225,\n",
       " 'wtd_std_ThermalConductivity': 3.8285152144058774,\n",
       " 'wtd_gmean_ElectronAffinity': 3.589116895521526,\n",
       " 'Ca': 3.410551989821297,\n",
       " 'wtd_entropy_ElectronAffinity': 3.394887646825719,\n",
       " 'wtd_std_atomic_mass': 3.196081179425849,\n",
       " 'wtd_entropy_atomic_mass': 2.8317915478142077,\n",
       " 'mean_ThermalConductivity': 2.7609890505042176,\n",
       " 'wtd_entropy_FusionHeat': 2.5880223369522963,\n",
       " 'gmean_ThermalConductivity': 2.524473443109448,\n",
       " 'range_atomic_radius': 2.524093411262746,\n",
       " 'Hg': 2.5220927025033046,\n",
       " 'wtd_range_ElectronAffinity': 2.5219111686153455,\n",
       " 'std_ElectronAffinity': 2.46421314705602,\n",
       " 'Tl': 2.4546386358442813,\n",
       " 'Ag': 2.302784507378022,\n",
       " 'std_atomic_mass': 2.2744357552364254,\n",
       " 'wtd_mean_atomic_radius': 2.237973420062889,\n",
       " 'wtd_std_atomic_radius': 2.2254238344081614,\n",
       " 'number_of_elements': 2.122047583186744,\n",
       " 'wtd_range_atomic_mass': 2.0528071077964123,\n",
       " 'range_ElectronAffinity': 2.005360667841686,\n",
       " 'wtd_entropy_ThermalConductivity': 2.000837021011696,\n",
       " 'Cl': 1.9297673269011162,\n",
       " 'wtd_entropy_atomic_radius': 1.9121479085864523,\n",
       " 'Ce': 1.8901282417445298,\n",
       " 'wtd_std_Density': 1.8754816171782072,\n",
       " 'entropy_ThermalConductivity': 1.8683344888813402,\n",
       " 'As': 1.8415773081693776,\n",
       " 'Nd': 1.8397797864841392,\n",
       " 'wtd_range_Density': 1.7605179907859752,\n",
       " 'mean_Density': 1.7353459958360868,\n",
       " 'wtd_entropy_fie': 1.7224005390498704,\n",
       " 'wtd_range_Valence': 1.6559624987557593,\n",
       " 'gmean_Density': 1.5920937181681887,\n",
       " 'wtd_std_FusionHeat': 1.5733640928211088,\n",
       " 'Si': 1.5648684463523095,\n",
       " 'wtd_range_atomic_radius': 1.5603023618190126,\n",
       " 'range_ThermalConductivity': 1.5006385187433908,\n",
       " 'Eu': 1.4879310575091271,\n",
       " 'entropy_atomic_mass': 1.4004485747792386,\n",
       " 'range_fie': 1.3949199273323565,\n",
       " 'mean_atomic_mass': 1.3902674557296972,\n",
       " 'wtd_mean_atomic_mass': 1.374116100908378,\n",
       " 'std_ThermalConductivity': 1.3479725189696785,\n",
       " 'B': 1.2676385029674593,\n",
       " 'Y': 1.267399150975522,\n",
       " 'mean_FusionHeat': 1.2397777333138105,\n",
       " 'entropy_ElectronAffinity': 1.2285906580168675,\n",
       " 'mean_ElectronAffinity': 1.1613942978972134,\n",
       " 'N': 1.152388533605237,\n",
       " 'gmean_atomic_radius': 1.0789354008968053,\n",
       " 'Lu': 1.0502294812614619,\n",
       " 'S': 1.0185770550728255,\n",
       " 'wtd_entropy_Valence': 0.9689201386204048,\n",
       " 'Mg': 0.9488433421154208,\n",
       " 'wtd_range_FusionHeat': 0.9288042033723887,\n",
       " 'wtd_std_ElectronAffinity': 0.9243360421323586,\n",
       " 'gmean_fie': 0.8945877730999369,\n",
       " 'entropy_Density': 0.8838734917345893,\n",
       " 'Ge': 0.8828450746666207,\n",
       " 'std_fie': 0.8737810087449167,\n",
       " 'std_Valence': 0.8431055330812769,\n",
       " 'Hf': 0.7984913127493036,\n",
       " 'Li': 0.796634524993242,\n",
       " 'O': 0.7911594370653423,\n",
       " 'entropy_fie': 0.7670593506228652,\n",
       " 'Co': 0.7484383138135025,\n",
       " 'Pt': 0.7179778930779498,\n",
       " 'range_FusionHeat': 0.6993648955431833,\n",
       " 'Gd': 0.678379550425078,\n",
       " 'Zn': 0.6659617240307223,\n",
       " 'std_atomic_radius': 0.6456580872402501,\n",
       " 'wtd_mean_Density': 0.639406500297488,\n",
       " 'wtd_gmean_atomic_radius': 0.6349529219528582,\n",
       " 'range_Valence': 0.6296974141258598,\n",
       " 'Al': 0.6143174974849611,\n",
       " 'P': 0.6104434335937566,\n",
       " 'Yb': 0.6080235920245366,\n",
       " 'wtd_mean_Valence': 0.593962899908123,\n",
       " 'Be': 0.5923453971085226,\n",
       " 'Sr': 0.5875302542168979,\n",
       " 'std_FusionHeat': 0.5859802259858727,\n",
       " 'Os': 0.5725433309843103,\n",
       " 'Pb': 0.5626411211616339,\n",
       " 'wtd_entropy_Density': 0.5364630842176473,\n",
       " 'entropy_Valence': 0.5236726357004029,\n",
       " 'Cu': 0.4991546792018496,\n",
       " 'mean_fie': 0.4918038098409781,\n",
       " 'Rb': 0.4773993180614497,\n",
       " 'entropy_atomic_radius': 0.47679260269786083,\n",
       " 'range_Density': 0.4673677750055407,\n",
       " 'K': 0.4583780022538765,\n",
       " 'I': 0.4345030482119355,\n",
       " 'gmean_ElectronAffinity': 0.4321163826977069,\n",
       " 'Er': 0.4278592979181801,\n",
       " 'mean_Valence': 0.424171105634465,\n",
       " 'Ni': 0.42145522494768306,\n",
       " 'wtd_std_fie': 0.4066037516335799,\n",
       " 'std_Density': 0.3545222777950935,\n",
       " 'Se': 0.34145384268933465,\n",
       " 'gmean_Valence': 0.32308332546547974,\n",
       " 'F': 0.32085371956738407,\n",
       " 'H': 0.2953803169665289,\n",
       " 'mean_atomic_radius': 0.2807679308097903,\n",
       " 'Tc': 0.27129610136661114,\n",
       " 'Dy': 0.2636761222276739,\n",
       " 'wtd_gmean_FusionHeat': 0.25240899697063646,\n",
       " 'gmean_atomic_mass': 0.2514192058843517,\n",
       " 'Mn': 0.25026069729193,\n",
       " 'W': 0.21782865620125816,\n",
       " 'Pd': 0.21323581881773215,\n",
       " 'wtd_gmean_Density': 0.20869211754225653,\n",
       " 'Ho': 0.20433725297667985,\n",
       " 'wtd_mean_fie': 0.19818411312445938,\n",
       " 'Au': 0.1973554324146537,\n",
       " 'Ti': 0.19291956206286173,\n",
       " 'wtd_gmean_fie': 0.192345901890139,\n",
       " 'Pr': 0.16702858977129323,\n",
       " 'wtd_gmean_Valence': 0.16432341792099697,\n",
       " 'Tb': 0.15947669311040277,\n",
       " 'La': 0.15544066245255456,\n",
       " 'Cd': 0.15505484468353997,\n",
       " 'Nb': 0.13505317534317976,\n",
       " 'V': 0.11956580487924759,\n",
       " 'Tm': 0.11811228993226089,\n",
       " 'Rh': 0.10230559755799083,\n",
       " 'Br': 0.09999983242007267,\n",
       " 'wtd_mean_ElectronAffinity': 0.09068023240862341,\n",
       " 'wtd_range_fie': 0.09038011428059288,\n",
       " 'entropy_FusionHeat': 0.08893691361185405,\n",
       " 'Ta': 0.08348592631428221,\n",
       " 'Sm': 0.0829435316009324,\n",
       " 'wtd_mean_FusionHeat': 0.08217011496926524,\n",
       " 'Cs': 0.0781204327529085,\n",
       " 'Fe': 0.06758399352255463,\n",
       " 'Sn': 0.06144331973007276,\n",
       " 'wtd_range_ThermalConductivity': 0.06129903065154236,\n",
       " 'Ru': 0.04617699829344316,\n",
       " 'Ga': 0.03935576023212859,\n",
       " 'Zr': 0.038878231080242194,\n",
       " 'In': 0.03319234984058856,\n",
       " 'Te': 0.03318108200549895,\n",
       " 'Cr': 0.02691993043988026,\n",
       " 'Sb': 0.024082020339043476,\n",
       " 'C': 0.021407473314095368,\n",
       " 'Sc': 0.020594831716496637,\n",
       " 'Na': 0.016942450459670483,\n",
       " 'Ir': 0.013371562036576768,\n",
       " 'wtd_gmean_atomic_mass': 0.013143277576432437,\n",
       " 'Re': 0.011506132232887174,\n",
       " 'Mo': 0.004963541124367912,\n",
       " 'gmean_FusionHeat': 0.002024485315723828}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_weights = {data.columns[k]:abs(v) for k, v in enumerate(gs_ridge.best_estimator_['ridge'].coef_)}\r\n",
    "dict(sorted(ridge_weights.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretability & Explainability\r\n",
    "Using at least one of your models above (if multiple were trained):\r\n",
    "\r\n",
    "- Which variable(s) was (were) \"\"most important\"\" and why?  How did you come to the conclusion and how should your audience interpret this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ba</th>\n",
       "      <th>wtd_gmean_ThermalConductivity</th>\n",
       "      <th>wtd_mean_ThermalConductivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.621979</td>\n",
       "      <td>61.015189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.619735</td>\n",
       "      <td>61.372331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.619095</td>\n",
       "      <td>60.943760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.620535</td>\n",
       "      <td>60.979474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.624878</td>\n",
       "      <td>61.086617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21258</th>\n",
       "      <td>0.00</td>\n",
       "      <td>95.001493</td>\n",
       "      <td>111.537778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21259</th>\n",
       "      <td>2.00</td>\n",
       "      <td>1.577047</td>\n",
       "      <td>108.680590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21260</th>\n",
       "      <td>0.00</td>\n",
       "      <td>57.038314</td>\n",
       "      <td>57.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21261</th>\n",
       "      <td>0.00</td>\n",
       "      <td>58.781651</td>\n",
       "      <td>59.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21262</th>\n",
       "      <td>0.00</td>\n",
       "      <td>12.919996</td>\n",
       "      <td>40.752000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21263 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Ba  wtd_gmean_ThermalConductivity  wtd_mean_ThermalConductivity\n",
       "0      0.20                       0.621979                     61.015189\n",
       "1      0.10                       0.619735                     61.372331\n",
       "2      0.10                       0.619095                     60.943760\n",
       "3      0.15                       0.620535                     60.979474\n",
       "4      0.30                       0.624878                     61.086617\n",
       "...     ...                            ...                           ...\n",
       "21258  0.00                      95.001493                    111.537778\n",
       "21259  2.00                       1.577047                    108.680590\n",
       "21260  0.00                      57.038314                     57.400000\n",
       "21261  0.00                      58.781651                     59.270000\n",
       "21262  0.00                      12.919996                     40.752000\n",
       "\n",
       "[21263 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n",
    "col= data[['Ba','wtd_gmean_ThermalConductivity','wtd_mean_ThermalConductivity' ]]\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.35"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate interquartile range for Ba\n",
    "q3, q1 = np.percentile(data['Ba'], [75 ,25])\n",
    "iqr_BA = q3 - q1\n",
    "#display interquartile range \n",
    "iqr_BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.220757864326025"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate interquartile range for Ba\n",
    "q3, q1 = np.percentile(data['wtd_gmean_ThermalConductivity'], [75 ,25])\n",
    "iqr_Ther_gmean = q3 - q1\n",
    "#display interquartile range \n",
    "iqr_Ther_gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.881957804581404"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate interquartile range for Ba\n",
    "q3, q1 = np.percentile(data['wtd_mean_ThermalConductivity'], [75 ,25])\n",
    "iqr_Ther_mean = q3 - q1\n",
    "#display interquartile range \n",
    "iqr_Ther_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use the Lasso model because the performance as explained above. The most important variable in the lasso model were the following:\n",
    "\n",
    "'Ba': 12.747910288530564,\n",
    "\n",
    "'wtd_gmean_ThermalConductivity': 12.364521222651712,\n",
    "\n",
    "'wtd_mean_ThermalConductivity': 11.947603500510484\n",
    "\n",
    "As you can see these 3 variables are the only variables among the 159 variables choosen to run the model that have a weight above 10. Therefore, the 3 mentioned variables are the most important variable to predict critical temperature by the lasso model.\n",
    "\n",
    "Because we had the necessity to normalize our data due to different distribution of values from the variables to provide appropriate variables to compare and work for the model, we interpret the variables accounting for our normalized data. We used the robust scalar for normalization. This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).\n",
    "Therefore, the interpretation of the most important 3 variables mentioned above are the following:\n",
    "BA= for any increase in 1.35 of the BA variables we will have an increase in the critical temperature of 12.75, when we keep constant the othe variables.\n",
    "Wtd_gmean_ThermalConductivity= for any increase in 46.22 of the wtd_gmean_ThermalConductivity variable we will have an increase in the critical temperature of 12.36, hen we keep constant the othe variables.\n",
    "Wtd_mean_ThermalConductivity= for any increase in 44.89 of the wtd_mean_ThermalConductivity variable we will have an increase in the critical temperature of 11.95, hen we keep constant the othe variables.\n",
    "\n",
    "The lasso model has MAE= 13.6; the general interpretation of this model for the user is that any prediction with this model will have a mean error of plus/minus 13.6.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Conclusions\r\n",
    "After all of your technical analysis and modeling; what are you proposing to your audience and why?  How should they view your results and what should they consider when moving forward?  Are there other approaches you'd recommend exploring?  This is where you \"bring it all home\" in language they understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will recommend to our audience to keep in mind that the most important variables to relate to critical high temperature are Ba wtd_gmean_ThermalConductivity, wtd_mean_ThermalConductivity. Because those variables turned out to have the highest weight in the lasso regularization model. So, moving forward the audience should concentrate in these 3 variables in the control or manipulation of critical high temperature. Furthermore, it is very reassurance that the same variables came out in the other models that we run as the variables with higher weight of importance. For the future, it can be done a prospective study looking the relation with of these 3 variables and critical high temperature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eaa798b471aa1a0109429a408b0faab53065248ef7f5a5989f90756771672ef6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
