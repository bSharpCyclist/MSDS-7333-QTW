{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries and reading in file\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#general sklearn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#NB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Files\n",
    "from os import listdir, getcwd, chdir\n",
    "from os.path import isfile, join, dirname, realpath\n",
    "import email\n",
    "import email.parser\n",
    "from email.parser import Parser\n",
    "\n",
    "#NLP\n",
    "import nltk\n",
    "import re \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "def NLP():\n",
    "    \n",
    "    get_cwd()\n",
    "    \n",
    "    directories = [\n",
    "            'easy_ham',\n",
    "            'easy_ham_2',\n",
    "            'hard_ham',\n",
    "            'spam',\n",
    "            'spam_2'\n",
    "        ]\n",
    "    \n",
    "    tex_dframe = pd.DataFrame()\n",
    "        \n",
    "    for d in directories:\n",
    "        mypath = getcwd() + '/data/' + d + '/'\n",
    "        onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    \n",
    "        for file in onlyfiles :\n",
    "            with open(mypath + file, encoding='latin1') as f:\n",
    "                body = f.read()\n",
    "                text = ' '.join(lines)\n",
    "                emails.append(text)\n",
    "                msg = email.message_from_string(str(body))\n",
    "                text= []\n",
    "                if msg.is_multipart():\n",
    "                    for payload in msg.get_payload():\n",
    "                        text.append(payload.get_payload())\n",
    "                    else:\n",
    "                        text.append(msg.get_payload())\n",
    "\n",
    "                        dframe = pd.DataFrame({'Body': text})\n",
    "                        tex = tex_dframe .append(dframe, ignore_index=True)\n",
    "\n",
    "    tex.to_csv('output_file.csv', index=False)\n",
    "    \n",
    "    return tex'' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_cwd():\n",
    "    try:\n",
    "        chdir(dirname(realpath(__file__)))\n",
    "    except:\n",
    "        chdir('/Users/fabiosavorgnan/Desktop/QTW/Week_1/Data/MSDS-7333-QTW')\n",
    "\n",
    "    active_dir = getcwd()\n",
    "       \n",
    "    return active_dir\n",
    "\n",
    "def main():\n",
    "    \n",
    "    get_cwd()\n",
    "    \n",
    "    directories = [\n",
    "            'easy_ham',\n",
    "            'easy_ham_2',\n",
    "            'hard_ham',\n",
    "            'spam',\n",
    "            'spam_2'\n",
    "        ]\n",
    "    \n",
    "    res_frame = pd.DataFrame()\n",
    "\n",
    "    # *dc - Added to keep a collection of email text\n",
    "    emails = []\n",
    "        \n",
    "    for d in directories:\n",
    "        mypath = getcwd() + '/data/' + d + '/'\n",
    "        onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    \n",
    "        try:\n",
    "            onlyfiles.remove('.DS_Store')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        for file in onlyfiles:\n",
    "            with open(mypath + file, encoding='latin1') as f:\n",
    "                lines = f.readlines()\n",
    "                f.close()\n",
    "                \n",
    "            in_reply_count = 0\n",
    "            sub_line_all_caps = 0\n",
    "            attachments = 0\n",
    "            subject_line = []\n",
    "            n_lines = 0\n",
    "            blank_lines = []\n",
    "            \n",
    "            for line in lines:\n",
    "\n",
    "                n_lines += 1\n",
    "                # append body of email to collection\n",
    "                text = ' '.join(lines)\n",
    "                emails.append(text)\n",
    "\n",
    "                if \"Subject: Re: \" in line:\n",
    "                   in_reply_count += 1\n",
    "                if \"Subject: \" in line:\n",
    "                   s_line = line.strip().replace('Subject: ','')\n",
    "                   s_line = ''.join(e for e in s_line if e.isalnum())\n",
    "                   num_upper = sum(1 for c in s_line if c.isupper())\n",
    "                   # append body of email to collection\n",
    "                   ttl_chars = len(s_line)\n",
    "                   if num_upper == ttl_chars:\n",
    "                       sub_line_all_caps += 1\n",
    "                   subject_line.append(s_line)\n",
    "                if \"content-type: multipart\" in line.lower():\n",
    "                   attachments += 1\n",
    "                if line == \"\\n\":\n",
    "                   blank_lines.append(n_lines)\n",
    "        \n",
    "            temp_frame = pd.DataFrame({\n",
    "                        'filename':file,\n",
    "                        'is_spam':['Y' if 'spam' in d else 'N'],\n",
    "                        'in_reply': ['Y' if in_reply_count > 0 else 'N'], \n",
    "                        'subj_caps': ['Y' if sub_line_all_caps > 0 else 'N'], \n",
    "                        'attachments': ['Y' if attachments > 0 else 'N'],\n",
    "                        ## *dc+3 \n",
    "                        #'body_lines': n_lines - min(blank_lines)\n",
    "                        'body_lines': [0 if len(blank_lines) == 0 else min(blank_lines)]\n",
    "                        }, index=[0])\n",
    "           \n",
    "            res_frame = res_frame.append(temp_frame, ignore_index=True)\n",
    "\n",
    "            ## *dc+2\n",
    "            # append body of email to collection (format below to add space between lines)\n",
    "            text = ' '.join(lines)\n",
    "            emails.append(text)\n",
    "            \n",
    "    res_frame.to_csv('output_file.csv', index=False)\n",
    "    \n",
    "    ## *dc - add emails and return a tuple\n",
    "    return res_frame, emails\n",
    "\n",
    "## *dc - Working from a notebook instead of py file.\n",
    "df, emails = main()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ########################################\n",
    "# ##### Main Function\n",
    "# ########################################    \n",
    "#if __name__ == \"__main__\":\n",
    "   # res_frame, emails = main()\n",
    "    #pass\n",
    "print(len(df),len(emails))\n",
    "\n",
    "print(emails[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lowercase and remove special characters to form a normalized document\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', ' ', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "\n",
    "    # tokenize document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    \n",
    "    # filter out stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Remove numbers\n",
    "    filtered_tokens = [token for token in filtered_tokens if not token.isdigit()]\n",
    "\n",
    "    # Remove short tokens\n",
    "    filtered_tokens = [token for token in filtered_tokens if len(token) > 2]\n",
    "\n",
    "    # stem tokens - Skipping for now\n",
    "    #filtered_tokens = [stemming.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    # re-create a normalized document\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_text = np.vectorize(normalize_document)\n",
    "norm_text = normalize_text(emails)\n",
    "\n",
    "print(type(norm_text),len(norm_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(ngram_range=(1,3), min_df=5, max_df=.8, stop_words=stop_words, norm='l2')\n",
    "tf_matrix = tf.fit_transform(norm_text)\n",
    "\n",
    "print(tf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv = CountVectorizer(min_df=0, max_df=1., stop_words=stop_words)\n",
    "cv_matrix = cv.fit_transform(norm_text)\n",
    "\n",
    "print(cv_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLUSTERS = 5\n",
    "km = KMeans(n_clusters=NUM_CLUSTERS, max_iter=10000, n_init=50, random_state=42).fit(tf_matrix)\n",
    "km\n",
    "\n",
    "df['kmeans_cluster'] = km.labels_\n",
    "\n",
    "email_clusters = (df[['directory', 'kmeans_cluster']]\n",
    "                  .sort_values(by=['kmeans_cluster'], \n",
    "                               ascending=False)\n",
    "                  .groupby('kmeans_cluster').head(20))  # top 20 movies for each cluster\n",
    "email_clusters = email_clusters.copy(deep=True)\n",
    "\n",
    "feature_names = tf.get_feature_names()\n",
    "topn_features = 50\n",
    "ordered_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "sample_silhouette_values = silhouette_samples(tf_matrix, km.labels_)\n",
    "\n",
    "# get key features for each cluster\n",
    "for cluster_num in range(NUM_CLUSTERS):\n",
    "\n",
    "    cluster_silhouette_values = sample_silhouette_values[km.labels_ == cluster_num]\n",
    "\n",
    "    key_features = [feature_names[index] \n",
    "                        for index in ordered_centroids[cluster_num, :topn_features]]\n",
    "    print('CLUSTER #'+str(cluster_num+1), \":\", cluster_silhouette_values.mean())\n",
    "    print('Cluster Size', cluster_silhouette_values.shape[0])\n",
    "    print('Key Features:', key_features)\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output a quick pivot table to see distribution of clusters vs spam/ham\n",
    "\n",
    "df.pivot_table(index='kmeans_cluster', columns='is_spam', values='directory', aggfunc='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "937ee2fabb34e01a2bfda78bec8fb2c2f970bd2ee382315ee43d2303ee1eab74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
